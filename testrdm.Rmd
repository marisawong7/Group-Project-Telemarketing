---
title: "Group Project Telemarketing"
author: "Juliet Niebylski, Ziying Peng, Leah Sun, Ally Weingarden, Marisa Wong"
output:
  html_document:
    toc: true
    theme: readable
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)
```

## Getting Data Ready for Analysis

```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))
```

## K-Means Clustering

```{r}
tele_norm_k_means <- tele_norm
tele_norm_k_means$yyes <- NULL
set.seed(12345) # Used same seed as professor
k_means_clusters <- kmeans(tele_norm_k_means, 5)
k_means_clusters$size
k_means_clusters$centers
tele_norm_k_means$yyes <- tele_norm$yyes
tele_norm_k_means$cluster <- k_means_clusters$cluster
summary(tele_norm_k_means)

# Calculate number of 1s for yyes for each cluster
num_yyes_1 <- sum(tele_norm_k_means[tele_norm_k_means$cluster == 1,]$yyes)
num_yyes_2 <- sum(tele_norm_k_means[tele_norm_k_means$cluster == 2,]$yyes)
num_yyes_3 <- sum(tele_norm_k_means[tele_norm_k_means$cluster == 3,]$yyes)
num_yyes_4 <- sum(tele_norm_k_means[tele_norm_k_means$cluster == 4,]$yyes)
num_yyes_5 <- sum(tele_norm_k_means[tele_norm_k_means$cluster == 5,]$yyes)

# Calculate the success rate for each cluster
(success_rate_1 <- num_yyes_1 / nrow(tele_norm_k_means[tele_norm_k_means$cluster == 1,]))
(success_rate_2 <- num_yyes_2 / nrow(tele_norm_k_means[tele_norm_k_means$cluster == 2,]))
(success_rate_3 <- num_yyes_3 / nrow(tele_norm_k_means[tele_norm_k_means$cluster == 3,]))
(success_rate_4 <- num_yyes_4 / nrow(tele_norm_k_means[tele_norm_k_means$cluster == 4,]))
(success_rate_5 <- num_yyes_5 / nrow(tele_norm_k_means[tele_norm_k_means$cluster == 5,]))
```

## Cluster 1

### Cluster 1 Training and Testing Data
```{r}
cluster_1_data <- tele_norm_k_means[tele_norm_k_means$cluster == 1,]
cluster_1_data$cluster <- NULL

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random_1 <- cluster_1_data[sample(nrow(cluster_1_data)),]

# Get 70% of data to be training data
n_train_1 <- round(nrow(cluster_1_data) * 0.7)
train_1 <- tele_random_1[1:n_train_1,]
# Get 30% of data to be testing data
test_1 <- tele_random_1[(n_train_1 + 1): nrow(cluster_1_data),]

```

### Cluster 1 Logistic Regression
```{r}
library(gmodels)
library(caret)

# Model 1:
# Create logistic regression model with all the predictor variables
glm_model_1 <- glm(formula = yyes ~ ., data = test_1, family = "binomial")
summary(glm_model_1)
# Get response values for testing data
prediction_values_glm1 <- predict(glm_model_1, newdata = test_1, type = "response")
# Get predictions for testing data
predictions_glm1 <- ifelse(prediction_values_glm1 > 0.2, 1, 0) # kappa of 0.1905 <- Use this threshold comparison
#predictions_glm1 <- ifelse(prediction_values_glm1 > 0.5, 1, 0) # kappa of 0.1354
#predictions_glm1 <- ifelse(prediction_values_glm1 > 0.15, 1, 0) # kappa of 0.1763
#predictions_glm1 <- ifelse(prediction_values_glm1 > 0.17, 1, 0) # kappa of 0.1824
#predictions_glm1 <- ifelse(prediction_values_glm1 > 0.22, 1, 0) # kappa of 0.184
#predictions_glm1 <- ifelse(prediction_values_glm1 > 0.19, 1, 0) # kappa of 0.1905
#predictions_glm1 <- ifelse(prediction_values_glm1 > 0.21, 1, 0) # kappa of 0.184
# Create Confusion Matrix
CrossTable(x = test_1$yyes, y = predictions_glm1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_glm1), as.factor(test_1$yyes), positive = "1")

# Model 2:
# New model with variables that are statistically significant at an alpha level of 0.1
glm_model2_1 <- glm(formula = yyes ~ + age + jobstudent + maritalmarried + educationbasic.6y
                    + educationbasic.9y + educationhigh.school 
                    + educationprofessional.course + educationuniversity.degree,
                    data = test_1, family = "binomial")
summary(glm_model2_1)
# Get response values for testing data
prediction_values_glm2_1 <- predict(glm_model2_1, newdata = test_1, type = "response") # Question: make as.vector?
summary(prediction_values_glm2_1)
# Get predictions for testing data
#predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.15, 1, 0) # Kappa of 0.0245
#predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.04, 1, 0) # Kappa of 0.0164
#predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.02, 1, 0) # Kappa of -1e-04
#predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.09, 1, 0) # Kappa of 0.0454
#predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.12, 1, 0) # Kappa of 0.0238
predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.07, 1, 0) # Kappa of 0.0566 <- Use this threshold comparison
#predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.08, 1, 0) # Kappa of 0.0566
#predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.06, 1, 0) # Kappa of 0.0562
#predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.075, 1, 0) # Kappa of 0.0465
# Create Confusion Matrix
CrossTable(x = test_1$yyes, y = predictions_glm2_1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_glm2_1), as.factor(test_1$yyes), positive = "1")

# Model 3:
# New model that does not include variables who have large p-values (close to 1)
glm_model3_1 <- glm(formula = yyes ~ . - jobretired - jobtechnician - jobtechnician - jobunemployed - educationilliterate
                    - monthaug - monthdec - monthjul - monthjun - monthmay - previous - emp.var.rate,
                    data = test_1, family = "binomial")
summary(glm_model3_1)
# Get response values for testing data
prediction_values_glm3_1 <- predict(glm_model2_1, newdata = test_1, type = "response")
summary(prediction_values_glm3_1)
# Get predictions for testing data
#predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.07, 1, 0) # Kappa of 0.1405
#predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.03, 1, 0) # Kappa of 0.0318
#predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.08, 1, 0) # Kappa of 0.1601
#predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.12, 1, 0) # Kappa of 0.1724
#predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.20, 1, 0) # Kappa of 0.1707
#predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.15, 1, 0) # Kappa of 0.184
#predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.17, 1, 0) # Kappa of 0.1905
#predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.18, 1, 0) # Kappa of 0.1921
predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.19, 1, 0) # Kappa of 0.1938 <- Use this threshold comparison
#predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.185, 1, 0) # Kappa of 0.1938
#predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.195, 1, 0) # Kappa of 0.1938
# Create Confusion Matrix
CrossTable(x = test_1$yyes, y = predictions_glm3_1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_glm3_1), as.factor(test_1$yyes), positive = "1")

# Note: glm_model3_1 is the best model of the three logistic regression models for cluster 1
#       since it has the largest kappa value of the three
```

## Cluster 1 ANN

```{r, cache = TRUE}

library(neuralnet)

# Model 1 (Baseline)
model1_1 <- neuralnet(formula = yyes ~ ., data = train_1)
plot(model1_1)
prediction1_1 <- predict(model1_1, newdata = test_1)
binaryprediction1_1 <- ifelse(prediction1_1 > 0.5, 1, 0)
library(gmodels)
library(caret)
# Create Confusion Matrix
CrossTable(x = test_1$yyes, y = binaryprediction1_1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction1_1), as.factor(test_1$yyes), positive = "1")
# Kappa 0.1215

# Model 2 (Adding Nodes)
model2_1 <- neuralnet(formula = yyes ~ ., data = train_1, hidden = 3)
plot(model2_1)
prediction2_1 <- predict(model2_1, newdata = test_1)
binaryprediction2_1 <- ifelse(prediction2_1 > 0.5, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_1$yyes, y = binaryprediction2_1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction2_1), as.factor(test_1$yyes), positive = "1")
# Kappa 0.0566

# Model 3 (Adding Layers)
model3_1 <- neuralnet(formula = yyes ~ ., data = train_1, hidden = c(2,3))
plot(model3_1)
prediction3_1 <- predict(model3_1, newdata = test_1)
binaryprediction3_1 <- ifelse(prediction3_1 > 0.5, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_1$yyes, y = binaryprediction3_1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction3_1), as.factor(test_1$yyes), positive = "1")
#Kappa 0.0685

# Model 4 (Adjusting Binary Prediction Definition)
model4_1 <- neuralnet(formula = yyes ~ ., data = train_1)
plot(model4_1)
prediction4_1 <- predict(model4_1, newdata = test_1)
binaryprediction4_1 <- ifelse(prediction4_1 > 0.4, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_1$yyes, y = binaryprediction4_1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction4_1), as.factor(test_1$yyes), positive = "1")
#Kappa 0.0979

# Model 5 (Adjusting Binary Prediction Definition)
model5_1 <- neuralnet(formula = yyes ~ ., data = train_1)
plot(model5_1)
prediction5_1 <- predict(model5_1, newdata = test_1)
binaryprediction5_1 <- ifelse(prediction5_1 > 0.3, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_1$yyes, y = binaryprediction5_1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction5_1), as.factor(test_1$yyes), positive = "1")
#Kappa 0.1378

# Model 6 (Adjusting Binary Prediction Definition)
model6_1 <- neuralnet(formula = yyes ~ ., data = train_1)
plot(model6_1)
prediction6_1 <- predict(model6_1, newdata = test_1)
binaryprediction6_1 <- ifelse(prediction6_1 > 0.2, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_1$yyes, y = binaryprediction6_1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction6_1), as.factor(test_1$yyes), positive = "1")
#Kappa 0.1616
```

## Cluster 1 KNN

```{r}
train_labels_1 <- train_1$yyes
test_labels_1 <- test_1$yyes
train_KNN_1 <- train_1
train_KNN_1$yyes <- NULL
test_KNN_1 <- test_1
test_KNN_1$yyes <- NULL
library(class)
library(caret)
#k_val1_1 <- round(sqrt(nrow(train_1))) # kappa of 0
#k_val1_1 <- 2 # kappa of -0.0073
k_val1_1 <- 3 # kappa of 0.0536 <- largest kappa found, use this k value
#k_val1_1 <- 4 # kappa of 0.0269
#k_val1_1 <- 5 # kappa of 0.0138
#k_val1_1 <- 10 # kappa of 0.0152
#k_val1_1 <- 20 # kappa of 0
#k_val1_1 <- 8 # kappa of 0.0152
#k_val1_1 <- 9 # kappa of 0.0152
#k_val1_1 <- 11 # kappa of 0.0152
#k_val1_1 <- 12 # kappa of 0.0152
#k_val1_1 <- 13 # kappa of 0
#k_val1_1 <- 30 # kappa of 0
test_pred1_1 <- knn(train = train_KNN_1, test = test_KNN_1, cl = train_labels_1, k = k_val1_1)
#Evaluate model results
library(gmodels)
CrossTable(x = test_labels_1, y = test_pred1_1, 
           prop.chisq=FALSE)
# Evaluate confusion matrix
confusionMatrix(test_pred1_1, as.factor(test_labels_1), positive = "1")
```

## Cluster 1 Combined Model

```{r}
# Add up each row of the three predictor vectors for the three chosen models
added_predictors_1 <- as.numeric(test_pred1_1) + binaryprediction6_1[,1] + predictions_glm3_1
predictions_combined_model_1 <- ifelse(added_predictors_1 >= 2, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_1$yyes, y = predictions_combined_model_1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_combined_model_1), as.factor(test_1$yyes), positive = "1")
```

## Cluster 2


### Cluster 2 Training and Testing Data
```{r}
cluster_2_data <- tele_norm_k_means[tele_norm_k_means$cluster == 2,]
cluster_2_data$cluster <- NULL

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random_2 <- cluster_2_data[sample(nrow(cluster_2_data)),]

# Get 70% of data to be training data
n_train_2 <- round(nrow(cluster_2_data) * 0.7)
train_2 <- tele_random_2[1:n_train_2,]
# Get 30% of data to be testing data
test_2 <- tele_random_2[(n_train_2 + 1): nrow(cluster_2_data),]

```

### Cluster 2 Logistic Regression
```{r}
library(gmodels)
library(caret)
# Create logistic regression model with all the predictor variables
glm_model_2 <- glm(formula = yyes ~ ., data = test_2, family = "binomial")
summary(glm_model_2)
# Get response values for testing data
prediction_values_glm2 <- predict(glm_model_2, newdata = test_2, type = "response")
# Get predictions for testing data
predictions_glm2 <- ifelse(prediction_values_glm2 > 0.5, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_2$yyes, y = predictions_glm2, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_glm2), as.factor(test_2$yyes), positive = "1")
```

## Cluster 2 ANN

```{r, cache = TRUE}

library(neuralnet)

model2_1 <- neuralnet(formula = yyes ~ ., data = train_2)
plot(model2_1)
prediction2_1 <- predict(model2_1, newdata = test_2)
binaryprediction2_1 <- ifelse(prediction2_1 > 0.5, 1, 0)
# Create Confusion Matrix
library(gmodels)
library(caret)
CrossTable(x = test_2$yyes, y = binaryprediction2_1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction2_1), as.factor(test_2$yyes), positive = "1")
```

## Cluster 2 KNN

```{r}
train_labels_2 <- train_2$yyes
test_labels_2 <- test_2$yyes
train_KNN_2 <- train_2
train_KNN_2$yyes <- NULL
test_KNN_2 <- test_2
test_KNN_2$yyes <- NULL
library(class)
library(caret)
# k_val1_1 <- round(sqrt(nrow(train_1))) 
k_val2_1 <-  2
test_pred2_1 <- knn(train = train_KNN_2, test = test_KNN_2, cl = train_labels_2, k = k_val2_1)
#Evaluate model results
library(gmodels)
CrossTable(x = test_labels_2, y = test_pred2_1, 
           prop.chisq=FALSE)
# Evaluate confusion matrix
confusionMatrix(test_pred2_1, as.factor(test_labels_2), positive = "1")
```

## Cluster 2 Combined Model

```{r}
# Add up each row of the three predictor vectors for the three chosen models
added_predictors_2 <- as.numeric(test_pred2_1) + binaryprediction2_1[,1] + predictions_glm2
predictions_combined_model2_1 <- ifelse(added_predictors_2 >= 2, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_2$yyes, y = predictions_combined_model2_1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_combined_model2_1), as.factor(test_2$yyes), positive = "1")
```



## Cluster 3

### Cluster 3 Training and Testing Data
```{r}
cluster_3_data <- tele_norm_k_means[tele_norm_k_means$cluster == 3,]
cluster_3_data$cluster <- NULL

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random_3 <- cluster_3_data[sample(nrow(cluster_3_data)),]

# Get 70% of data to be training data
n_train_3 <- round(nrow(cluster_3_data) * 0.7)
train_3 <- tele_random_3[1:n_train_3,]
# Get 30% of data to be testing data
test_3 <- tele_random_3[(n_train_3 + 1): nrow(cluster_3_data),]

```

### Cluster 3 Logistic Regression
```{r}
library(gmodels)
library(caret)
# Create logistic regression model with all the predictor variables
glm_model_3 <- glm(formula = yyes ~ ., data = test_3, family = "binomial")
summary(glm_model_3)
# Get response values for testing data
prediction_values_glm3 <- predict(glm_model_3, newdata = test_3, type = "response")
# Get predictions for testing data
predictions_glm3 <- ifelse(prediction_values_glm3 > 0.5, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_3$yyes, y = predictions_glm3, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_glm3), as.factor(test_3$yyes), positive = "1") 
# Kappa 0.3522

# Improve through setting NULL
test_3$maritalsingle <- NULL
test_3$educationbasic.9y <- NULL
test_3$housingyes <- NULL
test_3$previous <- NULL
test_3$euribor3m <- NULL
glm_model2_3 <- glm(formula = yyes ~ ., data = test_3, family = "binomial")
summary(glm_model2_3)
# Get response values for testing data
prediction_values_glm2_3 <- predict(glm_model2_3, newdata = test_3, type = "response")
# Get predictions for testing data
predictions_glm2_3 <- ifelse(prediction_values_glm2_3 > 0.5, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_3$yyes, y = predictions_glm2_3, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_glm2_3), as.factor(test_3$yyes), positive = "1")
# Kappa : 0.3587 
# A little higher than base model
```

## Cluster 3 ANN

```{r, cache = TRUE}

library(neuralnet)
# Model 1 (Base)
model1_3 <- neuralnet(formula = yyes ~ ., data = train_3)
plot(model1_3)
prediction1_3 <- predict(model1_3, newdata = test_3)
binaryprediction1_3 <- ifelse(prediction1_3 > 0.5, 1, 0)
# Create Confusion Matrix
library(gmodels)
library(caret)
CrossTable(x = test_3$yyes, y = binaryprediction1_3, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction1_3), as.factor(test_3$yyes), positive = "1")
# Kappa : 0.3079

# Model 2 (Adding Nodes)
model2_3 <- neuralnet(formula = yyes ~ ., data = train_3, hidden = 3)
plot(model2_3)
prediction2_3 <- predict(model2_3, newdata = test_3)
binaryprediction2_3 <- ifelse(prediction2_3 > 0.5, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_3$yyes, y = binaryprediction2_3, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction2_3), as.factor(test_3$yyes), positive = "1")
# Kappa 0.2232 (Lower)

# Model 3 (Adding Layers)
model3_3 <- neuralnet(formula = yyes ~ ., data = train_3, hidden = c(1,3))
plot(model3_3)
prediction3_3 <- predict(model3_3, newdata = test_3)
binaryprediction3_3 <- ifelse(prediction3_3 > 0.5, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_3$yyes, y = binaryprediction3_3, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction3_3), as.factor(test_3$yyes), positive = "1")
# Kappa : 0.3139 (higher than base model)

# Model 4 (Adjusting Binary Prediction Definition)
model4_3 <- neuralnet(formula = yyes ~ ., data = train_3)
plot(model4_3)
prediction4_3 <- predict(model4_3, newdata = test_3)
binaryprediction4_3 <- ifelse(prediction4_3 > 0.4, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_3$yyes, y = binaryprediction4_3, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction4_3), as.factor(test_3$yyes), positive = "1")
#Kappa 0.3715 (higher than base and layers)

# Model 5 (Adjusting Binary Prediction Definition)
model5_3 <- neuralnet(formula = yyes ~ ., data = train_3)
plot(model5_3)
prediction5_3 <- predict(model5_3, newdata = test_3)
binaryprediction5_3 <- ifelse(prediction5_3 > 0.3, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_3$yyes, y = binaryprediction5_3, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction5_3), as.factor(test_3$yyes), positive = "1")
#Kappa 0.3677 (lower than 0.4)

# Model 6 (Adjusting Binary Prediction Definition)
model6_3 <- neuralnet(formula = yyes ~ ., data = train_3)
plot(model6_3)
prediction6_3 <- predict(model6_3, newdata = test_3)
binaryprediction6_3 <- ifelse(prediction6_3 > 0.2, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_3$yyes, y = binaryprediction6_3, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction6_3), as.factor(test_3$yyes), positive = "1")
#Kappa 0.4071 (highest)
```

## Cluster 3 KNN

```{r}
train_labels_3 <- train_3$yyes
test_labels_3 <- test_3$yyes
train_KNN_3 <- train_3
train_KNN_3$yyes <- NULL
test_KNN_3 <- test_3
test_KNN_3$yyes <- NULL
library(class)
library(caret)
k_val1_3 <- 3 
test_pred1_3 <- knn(train = train_KNN_3, test = test_KNN_3, cl = train_labels_3, k = k_val1_3)
#Evaluate model results
library(gmodels)
CrossTable(x = test_labels_3, y = test_pred1_3, 
           prop.chisq=FALSE)
# Evaluate confusion matrix
confusionMatrix(test_pred1_3, as.factor(test_labels_3), positive = "1")
# Kappa : 0.2514  
```

## Cluster 3 Combined Model

```{r}
# Add up each row of the three predictor vectors for the three chosen models
added_predictors_3 <- as.numeric(test_pred1_3) + binaryprediction5_3[,1] + predictions_glm2_3
predictions_combined_model_3 <- ifelse(added_predictors_3 >= 2, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_3$yyes, y = predictions_combined_model_3, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_combined_model_3), as.factor(test_3$yyes), positive = "1")
# Kappa : 0.3742    
```

```{}
Conclusion for Cluster 3:
- Highest Kappa: 0.4071, using ANN (adjusting binary prediction to 0.2)
- KNN model is the least accurate one among all models (lowest Kappa)
- For Logistic Regression, Kappa is improved through setting insignificant variables to NULL, but really small
- ANN accuracy improves when we add layers but decreases when we add nodes
- 
```





## Cluster 5

### Cluster 5 Training and Testing Data
```{r}
cluster_5_data <- tele_norm_k_means[tele_norm_k_means$cluster == 1,]
cluster_5_data$cluster <- NULL

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random_5 <- cluster_5_data[sample(nrow(cluster_5_data)),]

# Get 70% of data to be training data
n_train_5 <- round(nrow(cluster_5_data) * 0.7)
train_5 <- tele_random_5[1:n_train_5,]
# Get 30% of data to be testing data
test_5 <- tele_random_5[(n_train_5 + 1) :nrow(cluster_5_data),]
```


### Cluster 5 Logistic Regression
```{r}
library(gmodels)
library(caret)
# Create logistic regression model with all the predictor variables
glm_model_5 <- glm(formula = yyes ~ ., data = test_5, family = "binomial")
summary(glm_model_5)
# Get response values for testing data
prediction_values_glm5 <- predict(glm_model_5, newdata = test_5, type = "response")
# Get predictions for testing data
predictions_glm5 <- ifelse(prediction_values_glm5 > 0.5, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_5$yyes, y = predictions_glm5, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_glm5), as.factor(test_5$yyes), positive = "1")
```


## Cluster 5 ANN

```{r, cache = TRUE}

library(neuralnet)

model1_5 <- neuralnet(formula = yyes ~ ., data = train_5)
plot(model1_5)
prediction1_5 <- predict(model1_5, newdata = test_5)
binaryprediction1_5 <- ifelse(prediction1_5 > 0.5, 1, 0)
# Create Confusion Matrix
library(gmodels)
library(caret)
CrossTable(x = test_5$yyes, y = binaryprediction1_5, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction1_5), as.factor(test_5$yyes), positive = "1")
```



### Cluster 5 kNN


```{r}
train_labels_5 <- train_5$yyes
test_labels_5 <- test_5$yyes
train_KNN_5 <- train_5
train_KNN_5$yyes <- NULL
test_KNN_5 <- test_5
test_KNN_5$yyes <- NULL
library(class)
library(caret)
# k_val1_5 <- round(sqrt(nrow(train_5))) 
k_val1_5 <- 3 
test_pred1_5 <- knn(train = train_KNN_5, test = test_KNN_5, cl = train_labels_5, k = k_val1_5)
#Evaluate model results
library(gmodels)
CrossTable(x = test_labels_5, y = test_pred1_5, 
           prop.chisq=FALSE)
# Evaluate confusion matrix
confusionMatrix(test_pred1_5, as.factor(test_labels_5), positive = "1")
```


## Cluster 5 Combined Model

```{r}
# Add up each row of the three predictor vectors for the three chosen models
added_predictors_5 <- as.numeric(test_pred1_5) + binaryprediction1_5[,1] + predictions_glm5
predictions_combined_model_5 <- ifelse(added_predictors_5 >= 2, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_5$yyes, y = predictions_combined_model_5, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_combined_model_5), as.factor(test_5$yyes), positive = "1")
```









=======
## Conclusion

