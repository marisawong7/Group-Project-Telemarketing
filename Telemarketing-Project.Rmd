---
title: "Group Project Telemarketing"
author: "Juliet Niebylski, Ziying Peng, Leah Sun, Ally Weingarden, Marisa Wong"
output:
  html_document:
    toc: true
    theme: readable
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


NOTE: 
- to compare models, look at the sensitivity and pos pred values (PPV)
- we want a aim for a sensitivity of at least 0.167 and larger PPV values (but try to balance the two: have both large sensitivity and large PPV)
- the sensitivity is the success rate
- Info about PPV here: https://www.timeofcare.com/positive-predictive-value/
- Info about calculation of sensitivity and PPV can be found using the following command in R: ?confusionMatrix


## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)
```

## Getting Data Ready for Analysis

```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))
```

## K-Means Clustering

```{r}
tele_norm_k_means <- tele_norm
tele_norm_k_means$yyes <- NULL
set.seed(12345) # Used same seed as professor
k_means_clusters <- kmeans(tele_norm_k_means, 5)
k_means_clusters$size
k_means_clusters$centers
tele_norm_k_means$yyes <- tele_norm$yyes
tele_norm_k_means$cluster <- k_means_clusters$cluster
summary(tele_norm_k_means)

# Calculate number of 1s for yyes for each cluster
num_yyes_1 <- sum(tele_norm_k_means[tele_norm_k_means$cluster == 1,]$yyes)
num_yyes_2 <- sum(tele_norm_k_means[tele_norm_k_means$cluster == 2,]$yyes)
num_yyes_3 <- sum(tele_norm_k_means[tele_norm_k_means$cluster == 3,]$yyes)
num_yyes_4 <- sum(tele_norm_k_means[tele_norm_k_means$cluster == 4,]$yyes)
num_yyes_5 <- sum(tele_norm_k_means[tele_norm_k_means$cluster == 5,]$yyes)

# Calculate the success rate for each cluster
(success_rate_1 <- num_yyes_1 / nrow(tele_norm_k_means[tele_norm_k_means$cluster == 1,]))
(success_rate_2 <- num_yyes_2 / nrow(tele_norm_k_means[tele_norm_k_means$cluster == 2,]))
(success_rate_3 <- num_yyes_3 / nrow(tele_norm_k_means[tele_norm_k_means$cluster == 3,]))
(success_rate_4 <- num_yyes_4 / nrow(tele_norm_k_means[tele_norm_k_means$cluster == 4,]))
(success_rate_5 <- num_yyes_5 / nrow(tele_norm_k_means[tele_norm_k_means$cluster == 5,]))
```

## Cluster 1

### Cluster 1 Training and Testing Data
```{r}
cluster_1_data <- tele_norm_k_means[tele_norm_k_means$cluster == 1,]
cluster_1_data$cluster <- NULL

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random_1 <- cluster_1_data[sample(nrow(cluster_1_data)),]

# Get 70% of data to be training data
n_train_1 <- round(nrow(cluster_1_data) * 0.7)
train_1 <- tele_random_1[1:n_train_1,]
# Get 30% of data to be testing data
test_1 <- tele_random_1[(n_train_1 + 1): nrow(cluster_1_data),]

```

### Cluster 1 Logistic Regression
```{r}
library(gmodels)
library(caret)

# Model 1:
# Create logistic regression model with all the predictor variables
glm_model_1 <- glm(formula = yyes ~ ., data = test_1, family = "binomial")
summary(glm_model_1)
# Get response values for testing data
prediction_values_glm1 <- predict(glm_model_1, newdata = test_1, type = "response")
# Get predictions for testing data

# sensitivity of 0.128, Pos Pred Value of 0.50 <- Use this threshold comparison (note: sensitivity < 0.167)
# num_calls_made_1 = 32, success_rate_cluster1 = 0.5, 
# profit_per_100_1 = 2.072539, profit_1 = 64 for this threshold comparison:
predictions_glm1 <- ifelse(prediction_values_glm1 > 0.2, 1, 0)

#predictions_glm1 <- ifelse(prediction_values_glm1 > 0.5, 1, 0) # sensitivity of 0.08, Pos Pred Value of 0.666667
#predictions_glm1 <- ifelse(prediction_values_glm1 > 0.15, 1, 0) # sensitivity of 0.128, Pos Pred Value of 0.390244
#predictions_glm1 <- ifelse(prediction_values_glm1 > 0.17, 1, 0) # sensitivity of 0.128, Pos Pred Value of 0.432432
# predictions_glm1 <- ifelse(prediction_values_glm1 > 0.22, 1, 0) # sensitivity of 0.120, Pos Pred Value of 0.535714
#predictions_glm1 <- ifelse(prediction_values_glm1 > 0.19, 1, 0) # sensitivity of 0.128, Pos Pred Value of 0.50
#predictions_glm1 <- ifelse(prediction_values_glm1 > 0.21, 1, 0) # sensitivity of 0.120, Pos Pred Value of 0.535714
# Create Confusion Matrix
CrossTable(x = test_1$yyes, y = predictions_glm1, propsq = FALSE)
# Evaluate confusion matrix
(confusion_matrix_glm1_1 <- confusionMatrix(as.factor(predictions_glm1), as.factor(test_1$yyes), positive = "1"))

# # Model 2:
# # New model with variables that are statistically significant at an alpha level of 0.1
# glm_model2_1 <- glm(formula = yyes ~ + age + jobstudent + maritalmarried + educationbasic.6y
#                     + educationbasic.9y + educationhigh.school 
#                     + educationprofessional.course + educationuniversity.degree,
#                     data = test_1, family = "binomial")
# summary(glm_model2_1)
# # Get response values for testing data
# prediction_values_glm2_1 <- predict(glm_model2_1, newdata = test_1, type = "response")
# summary(prediction_values_glm2_1)
# # Get predictions for testing data
# #predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.15, 1, 0) # sensitivity of 0.0160, Pos Pred Value of 0.2222222
# #predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.04, 1, 0) # sensitivity of 0.50400, Pos Pred Value of 0.04880
# #predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.02, 1, 0) # sensitivity of 0.9920, Pos Pred Value of 0.040444
# #predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.09, 1, 0) # sensitivity of 0.0320, Pos Pred Value of 0.210526
# #predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.12, 1, 0) # sensitivity of 0.0160, Pos Pred Value of 0.20
# #predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.07, 1, 0) # sensitivity of 0.0560, Pos Pred Value of 0.10
# #predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.08, 1, 0) # sensitivity of 0.0480, Pos Pred Value of 0.162162
# 
# # sensitivity of 0.1040, Pos Pred Value of 0.09091 <- use this threshold comparison
# # num_calls_made_1 = 143, success_rate_cluster1 = 0.09090909,
# # profit_per_100_1 = -2.104922, profit_1 = -65 for this threshold comparison of 0.06:
# predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.06, 1, 0)
# 
# #predictions_glm2_1 <- ifelse(prediction_values_glm2_1 > 0.075, 1, 0) # sensitivity of 0.0480, Pos Pred Value of 0.120
# # Create Confusion Matrix
# CrossTable(x = test_1$yyes, y = predictions_glm2_1, propsq = FALSE)
# # Evaluate confusion matrix
# (confusion_matrix_glm2_1 <- confusionMatrix(as.factor(predictions_glm2_1), as.factor(test_1$yyes), positive = "1"))
# 
# # Model 3:
# # New model that does not include variables who have large p-values (close to 1)
# glm_model3_1 <- glm(formula = yyes ~ . - jobretired - jobtechnician - jobtechnician - jobunemployed - educationilliterate
#                     - monthaug - monthdec - monthjul - monthjun - monthmay - previous - emp.var.rate,
#                     data = test_1, family = "binomial")
# summary(glm_model3_1)
# # Get response values for testing data
# prediction_values_glm3_1 <- predict(glm_model2_1, newdata = test_1, type = "response")
# summary(prediction_values_glm3_1)
# # Get predictions for testing data
# #predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.07, 1, 0) # sensitivity of 0.0560, Pos Pred Value of 0.10
# 
# # sensitivity of 0.8960, Pos Pred Value of 0.04199
# # (note: could be overfitting the model to the test data since the sensitivity is high while pos pred value is low)
# # num_calls_made_1 = 2667, success_rate_cluster1 = 0.04199475, profit_per_100_1 = -64.60492, profit_1 = -1995
# # for this threshold comparison of 0.03:
# #predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.03, 1, 0)
# 
# #predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.08, 1, 0) # sensitivity of 0.0480, Pos Pred Value of 0.162162
# #predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.12, 1, 0) # sensitivity of 0.0160, Pos Pred Value of 0.20
# 
# # sensitivity of 0.0160, Pos Pred Value of 0.3333333 <- use this threshold comparison
# # num_calls_made_1 = 6, success_rate_cluster1 = 0.3333333,
# # profit_per_100_1 = 0.1943005, profit_1 = 6
# predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.20, 1, 0) 
# #predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.15, 1, 0) # sensitivity of 0.0160, Pos Pred Value of 0.2222222
# #predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.17, 1, 0) # sensitivity of 0.0160, Pos Pred Value of 0.250
# #predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.18, 1, 0) # sensitivity of 0.0160, Pos Pred Value of 0.250
# #predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.19, 1, 0) # sensitivity of 0.0160, Pos Pred Value of 0.2857143
# #predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.185, 1, 0) # sensitivity of 0.0160, Pos Pred Value of 0.2857143
# #predictions_glm3_1 <- ifelse(prediction_values_glm3_1 > 0.195, 1, 0) # sensitivity of 0.0160, Pos Pred Value of 0.3333333
# # Create Confusion Matrix
# CrossTable(x = test_1$yyes, y = predictions_glm3_1, propsq = FALSE)
# # Evaluate confusion matrix
# (confusion_matrix_glm3_1 <- confusionMatrix(as.factor(predictions_glm3_1), as.factor(test_1$yyes), positive = "1"))

# Note: glm_model1_1 is the best model of the three logistic regression models for cluster 1
#       since it has the largest profit per 100 people of the logistic regression models 
#       for the thresholds that had profit per 100 people calculated
```

## Cluster 1 ANN

```{r, cache = TRUE}

library(neuralnet)

# # Model 1 (Baseline)
# model1_1 <- neuralnet(formula = yyes ~ ., data = train_1)
# plot(model1_1)
# prediction1_1 <- predict(model1_1, newdata = test_1)
# binaryprediction1_1 <- ifelse(prediction1_1 > 0.5, 1, 0)
# library(gmodels)
# library(caret)
# # Create Confusion Matrix
# CrossTable(x = test_1$yyes, y = binaryprediction1_1, propsq = FALSE)
# # Evaluate confusion matrix
# (confusion_matrix_model1_ANN1 <- confusionMatrix(as.factor(binaryprediction1_1), as.factor(test_1$yyes), positive = "1"))
# # Sensitivity: 0.056000; Pos Pred Value: 0.388889

# Model 2 (Adding Nodes)
model2_1 <- neuralnet(formula = yyes ~ ., data = train_1, hidden = 3)
plot(model2_1)
prediction2_1 <- predict(model2_1, newdata = test_1)
binaryprediction2_1 <- ifelse(prediction2_1 > 0.5, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_1$yyes, y = binaryprediction2_1, propsq = FALSE)
# Evaluate confusion matrix
(confusion_matrix_model2_ANN1 <- confusionMatrix(as.factor(binaryprediction2_1), as.factor(test_1$yyes), positive = "1"))
# Sensitivity: 0.088000; Pos Pred Value: 0.423077


# # Model 3 (Adding Layers)
# model3_1 <- neuralnet(formula = yyes ~ ., data = train_1, hidden = c(2,3), threshold = 0.01, stepmax = 10^5)
# plot(model3_1)
# prediction3_1 <- predict(model3_1, newdata = test_1)
# binaryprediction3_1 <- ifelse(prediction3_1 > 0.5, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_1$yyes, y = binaryprediction3_1, propsq = FALSE)
# # Evaluate confusion matrix
# (confusion_matrix_model3_ANN1 <-confusionMatrix(as.factor(binaryprediction3_1), as.factor(test_1$yyes), positive = "1"))
# #Sensitivity: 0.040000; Pos Pred Value: 0.384615   

# # Model 4 (Adjusting Binary Prediction Definition)
# model4_1 <- neuralnet(formula = yyes ~ ., data = train_1)
# plot(model4_1)
# prediction4_1 <- predict(model4_1, newdata = test_1)
# binaryprediction4_1 <- ifelse(prediction4_1 > 0.4, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_1$yyes, y = binaryprediction4_1, propsq = FALSE)
# # Evaluate confusion matrix
# (confusion_matrix_model4_ANN1 <-confusionMatrix(as.factor(binaryprediction4_1), as.factor(test_1$yyes), positive = "1"))
# #Sensitivity: 0.0240000; Pos Pred Value: 0.3333333

# # Model 5 (Adjusting Binary Prediction Definition)
# model5_1 <- neuralnet(formula = yyes ~ ., data = train_1, threshold = 0.01, stepmax = 10^5)
# plot(model5_1)
# prediction5_1 <- predict(model5_1, newdata = test_1)
# binaryprediction5_1 <- ifelse(prediction5_1 > 0.3, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_1$yyes, y = binaryprediction5_1, propsq = FALSE)
# # Evaluate confusion matrix
# (confusion_matrix_model5_ANN1 <-confusionMatrix(as.factor(binaryprediction5_1), as.factor(test_1$yyes), positive = "1"))
# #Sensitivity: 0.072000; Pos Pred Value: 0.642857  

# # Model 6 (Adjusting Binary Prediction Definition)
# model6_1 <- neuralnet(formula = yyes ~ ., data = train_1)
# plot(model6_1)
# prediction6_1 <- predict(model6_1, newdata = test_1)
# binaryprediction6_1 <- ifelse(prediction6_1 > 0.2, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_1$yyes, y = binaryprediction6_1, propsq = FALSE)
# # Evaluate confusion matrix
# (confusion_matrix_model6_ANN1 <-confusionMatrix(as.factor(binaryprediction6_1), as.factor(test_1$yyes), positive = "1"))
# #Sensitivity: 0.048000; Pos Pred Value: 0.428571   
```

## Cluster 1 KNN

```{r}
train_labels_1 <- train_1$yyes
test_labels_1 <- test_1$yyes
train_KNN_1 <- train_1
train_KNN_1$yyes <- NULL
test_KNN_1 <- test_1
test_KNN_1$yyes <- NULL
library(class)
library(caret)
#k_val1_1 <- round(sqrt(nrow(train_1))) # sensitivity of 0.00000, Pos Pred Value of NaN
#k_val1_1 <- 2 # sensitivity of 0.0240, Pos Pred Value of 0.0322581

# sensitivity of 0.0320, Pos Pred Value of 0.40 <- Use this k value (note: sensitivity is < 0.167)
# num_calls_made_1_knn = 10, success_rate_cluster1_knn = 0.4, 
# profit_per_100_1 = 0.4533679, profit_1_knn = 14
k_val1_1 <- 3 

#k_val1_1 <- 4 # sensitivity of 0.0160, Pos Pred Value of 0.9600909
#k_val1_1 <- 5 # sensitivity of 0.0080, Pos Pred Value of 0.3333333
#k_val1_1 <- 10 # sensitivity of 0.0080, Pos Pred Value of 1.0
#k_val1_1 <- 20 # sensitivity of 0.0, Pos Pred Value of NaN
#k_val1_1 <- 8 # sensitivity of 0.0080, Pos Pred Value of 1.0
#k_val1_1 <- 9 # sensitivity of 0.0080, Pos Pred Value of 1.0
#k_val1_1 <- 11 # sensitivity of 0.0080, Pos Pred Value of 1.0
#k_val1_1 <- 12 # sensitivity of 0.0080, Pos Pred Value of 1.0
#k_val1_1 <- 13 # sensitivity of 0.0, Pos Pred Value of NaN
#k_val1_1 <- 30 # sensitivity of 0.0, Pos Pred Value of NaN
test_pred1_1 <- knn(train = train_KNN_1, test = test_KNN_1, cl = train_labels_1, k = k_val1_1)
#Evaluate model results
library(gmodels)
CrossTable(x = test_labels_1, y = test_pred1_1, 
           prop.chisq=FALSE)
# Evaluate confusion matrix
(confusion_matrix_knn_1 <- confusionMatrix(test_pred1_1, as.factor(test_labels_1), positive = "1"))
```

## Cluster 1 Combined Model

```{r}
# Add up each row of the three predictor vectors for the three chosen models
added_predictors_1 <- as.numeric(test_pred1_1) + binaryprediction2_1[,1] + predictions_glm1
predictions_combined_model_1 <- ifelse(added_predictors_1 >= 2, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_1$yyes, y = predictions_combined_model_1, propsq = FALSE)
# Evaluate confusion matrix
(combined_model_confusionMatrix_1 <- confusionMatrix(as.factor(predictions_combined_model_1), as.factor(test_1$yyes), positive = "1"))
```

## Cluster 1 ANN Profit Calculations

```{r}
# 1: Get number of potential calls (number of values in the test set of this cluster)
(num_potential_calls_ANN1 <- nrow(test_1))
# 3088 in test set

# 2: Get number of people we will call in this cluster 
# (number of people we predicted yyes to be 1 in this cluster)
# Add up values predicted to be 1 (from confusion matrix of chosen model for this cluster):
(num_calls_made_ANN1 <- confusion_matrix_model2_ANN1$table[2] + confusion_matrix_model2_ANN1$table[4])
# 26 calls made

# 3: Calculate the success rate (number of people we will call 
#    (model predicted yyes to be 1) who actually have a yyes value of 1 
#    divided by the number of people we will call)
predicted_correctly_1s <- confusion_matrix_model2_ANN1$table[4]
(success_rate_clusterANN1 <- predicted_correctly_1s / num_calls_made_ANN1)
#0.4230769

# 4: Calculate the profit per 100 people:
# (100 * (# calls made / # potential calls made) * success rate * 6) -
# (100 * (# calls made/# potential calls made) * 1))
(profit_per_100_ANN1 <- (100 * (num_calls_made_ANN1 / num_potential_calls_ANN1)
                         * success_rate_clusterANN1 * 6) - 
                         (100 * (num_calls_made_ANN1 / num_potential_calls_ANN1) * 1))
#1.295337

# 5: Also, calculate what the profit is for this chosen model of this cluster
(profit_ANN1 <- (num_calls_made_ANN1 * success_rate_clusterANN1 * 6) - (num_calls_made_ANN1 * (num_calls_made_ANN1 / num_potential_calls_ANN1) * 1))
#65.78109
```

## Cluster 1 Additional Data Calculated for Logistic Regression

```{r}
# Calculating additional data for logistic regression model
# Note: did run the code for all three logistic regression models 
# for certain thresholds but only glm_model_1 is shown since it has the 
# largest profit per 100 people out of the other logistic regression models

# 1: Get number of potential calls (number of values in the test set of this cluster)
(num_potential_calls_1 <- nrow(test_1))

# 2: Get number of people we will call in this cluster 
# (number of people we predicted yyes to be 1 in this cluster)
# Add up values predicted to be 1 (from confusion matrix of chosen model for this cluster):
(num_calls_made_1 <- confusion_matrix_glm1_1$table[2] + confusion_matrix_glm1_1$table[4])

# 3: Calculate the success rate (number of people we will call 
#    (model predicted yyes to be 1) who actually have a yyes value of 1 
#    divided by the number of people we will call)
(success_rate_cluster1 <- confusion_matrix_glm1_1$table[4] / num_calls_made_1)

# 4: Calculate the profit per 100 people:
# (100 * (# calls made / # potential calls made) * success rate * 6) -
# (100 * (# calls made / # potential calls made) * 1)
(profit_per_100_1 <- (100 * (num_calls_made_1 / num_potential_calls_1) 
                      * success_rate_cluster1 * 6) - 
                      (100 * (num_calls_made_1 / num_potential_calls_1) * 1))

# 5: Also, calculate what the profit is for this chosen model of this cluster:
# (# calls made * success rate * 6) - (# calls made * 1)
(profit_1 <- (num_calls_made_1 * success_rate_cluster1 * 6) - (num_calls_made_1 * 1))

# Note: This model has the largest profit per 100 people 
# compared to ANN, KNN, and the combined model
# Therefore, we will use this model for cluster 1
```

## Cluster 1 Additional Data Calculated for KNN

```{r}
# Calculating additional data for KNN model

# 1: Get number of potential calls (number of values in the test set of this cluster)
(num_potential_calls_1_knn <- nrow(test_1))

# 2: Get number of people we will call in this cluster 
# (number of people we predicted yyes to be 1 in this cluster)
# Add up values predicted to be 1 (from confusion matrix of chosen model for this cluster):
(num_calls_made_1_knn <- confusion_matrix_knn_1$table[2] + confusion_matrix_knn_1$table[4])

# 3: Calculate the success rate (number of people we will call 
#    (model predicted yyes to be 1) who actually have a yyes value of 1 
#    divided by the number of people we will call)
(success_rate_cluster1_knn <- confusion_matrix_knn_1$table[4] / num_calls_made_1_knn)

# 4: Calculate the profit per 100 people:
# (100 * (# calls made / # potential calls made) * success rate * 6) -
# (100 * (# calls made / # potential calls made) * 1)
(profit_per_100_1 <- (100 * (num_calls_made_1_knn / num_potential_calls_1_knn) 
                      * success_rate_cluster1_knn * 6) - 
                      (100 * (num_calls_made_1_knn / num_potential_calls_1_knn) * 1))

# 5: Also, calculate what the profit is for this chosen model of this cluster:
# (# calls made * success rate * 6) - (# calls made * 1)
(profit_1_knn <- (num_calls_made_1_knn * success_rate_cluster1_knn * 6) - (num_calls_made_1_knn * 1))
```

## Cluster 1 Additional Data Calculated for Combined Model

```{r}
# Calculating additional data for combined model

# 1: Get number of potential calls (number of values in the test set of this cluster)
(num_potential_calls_1_combined <- nrow(test_1))

# 2: Get number of people we will call in this cluster 
# (number of people we predicted yyes to be 1 in this cluster)
# Add up values predicted to be 1 (from confusion matrix of chosen model for this cluster):
(num_calls_made_1_combined <- combined_model_confusionMatrix_1$table[2] + combined_model_confusionMatrix_1$table[4])

# 3: Calculate the success rate (number of people we will call 
#    (model predicted yyes to be 1) who actually have a yyes value of 1 
#    divided by the number of people we will call)
(success_rate_cluster1_combined <- combined_model_confusionMatrix_1$table[4] / num_calls_made_1_combined)

# 4: Calculate the profit per 100 people:
# (100 * (# calls made / # potential calls made) * success rate * 6) -
# (100 * (# calls made / # potential calls made) * 1)
(profit_per_100_1 <- (100 * (num_calls_made_1_combined / num_potential_calls_1_combined) 
                      * success_rate_cluster1_combined * 6) - 
                      (100 * (num_calls_made_1_combined / num_potential_calls_1_combined) * 1))

# 5: Also, calculate what the profit is for this chosen model of this cluster:
# (# calls made * success rate * 6) - (# calls made * 1)
(profit_1_combined <- (num_calls_made_1_combined * success_rate_cluster1_combined * 6) - (num_calls_made_1_combined * 1))
```

## Conclusion for Cluster 1

For cluster 1, we have chosen a logistic regression model since it has the largest profit per 100 people ($2.072539). In addition, the chosen logistic regression model for cluster 1 indicates to call 32 people, so it will not be very time consuming to call all these people.

## Cluster 2


### Cluster 2 Training and Testing Data
```{r}
cluster_2_data <- tele_norm_k_means[tele_norm_k_means$cluster == 2,]
cluster_2_data$cluster <- NULL

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random_2 <- cluster_2_data[sample(nrow(cluster_2_data)),]

# Get 70% of data to be training data
n_train_2 <- round(nrow(cluster_2_data) * 0.7)
train_2 <- tele_random_2[1:n_train_2,]
# Get 30% of data to be testing data
test_2 <- tele_random_2[(n_train_2 + 1): nrow(cluster_2_data),]

```

### Cluster 2 Logistic Regression
```{r}
library(gmodels)
library(caret)
# Create logistic regression model with all the predictor variables
glm_model_2 <- glm(formula = yyes ~ ., data = test_2, family = "binomial")
summary(glm_model_2)
# Get response values for testing data
prediction_values_glm2 <- predict(glm_model_2, newdata = test_2, type = "response")
# Get predictions for testing data
predictions_glm2 <- ifelse(prediction_values_glm2 > 0.5, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_2$yyes, y = predictions_glm2, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_glm2), as.factor(test_2$yyes), positive = "1")
```

## Cluster 2 ANN

```{r, cache = TRUE}

library(neuralnet)

model2_1 <- neuralnet(formula = yyes ~ ., data = train_2)
plot(model2_1)
prediction2_1 <- predict(model2_1, newdata = test_2)
binaryprediction2_1 <- ifelse(prediction2_1 > 0.5, 1, 0)
# Create Confusion Matrix
library(gmodels)
library(caret)
CrossTable(x = test_2$yyes, y = binaryprediction2_1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction2_1), as.factor(test_2$yyes), positive = "1")
```

## Cluster 2 KNN

```{r}
train_labels_2 <- train_2$yyes
test_labels_2 <- test_2$yyes
train_KNN_2 <- train_2
train_KNN_2$yyes <- NULL
test_KNN_2 <- test_2
test_KNN_2$yyes <- NULL
library(class)
library(caret)
# k_val1_1 <- round(sqrt(nrow(train_1))) 
k_val2_1 <-  2
test_pred2_1 <- knn(train = train_KNN_2, test = test_KNN_2, cl = train_labels_2, k = k_val2_1)
#Evaluate model results
library(gmodels)
CrossTable(x = test_labels_2, y = test_pred2_1, 
           prop.chisq=FALSE)
# Evaluate confusion matrix
confusionMatrix(test_pred2_1, as.factor(test_labels_2), positive = "1")
```

## Cluster 2 Combined Model

```{r}
# Add up each row of the three predictor vectors for the three chosen models
added_predictors_2 <- as.numeric(test_pred2_1) + binaryprediction2_1[,1] + predictions_glm2
predictions_combined_model2_1 <- ifelse(added_predictors_2 >= 2, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_2$yyes, y = predictions_combined_model2_1, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_combined_model2_1), as.factor(test_2$yyes), positive = "1")
```


## Cluster 3

### Cluster 3 Training and Testing Data

```{r}
cluster_3_data <- tele_norm_k_means[tele_norm_k_means$cluster == 3,]
cluster_3_data$cluster <- NULL

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random_3 <- cluster_3_data[sample(nrow(cluster_3_data)),]

# Get 70% of data to be training data
n_train_3 <- round(nrow(cluster_3_data) * 0.7)
train_3 <- tele_random_3[1:n_train_3,]
# Get 30% of data to be testing data
test_3 <- tele_random_3[(n_train_3 + 1): nrow(cluster_3_data),]

```

### Cluster 3 Logistic Regression
```{r}
library(gmodels)
library(caret)
# # Model 1
# # Create logistic regression model with all the predictor variables
# glm_model1_3 <- glm(formula = yyes ~ ., data = test_3, family = "binomial")
# summary(glm_model1_3)
# # Get response values for testing data
# prediction_values_glm1_3 <- predict(glm_model1_3, newdata = test_3, type = "response")
# predictions_glm1_3 <- ifelse(prediction_values_glm1_3 > 0.4, 1, 0)
# # sensitivity: 0.44619 (>0.167); pos pred: 0.59050, use this model
# # predictions_glm1_3 <- ifelse(prediction_values_glm1_3 > 0.5, 1, 0)
# # sensitivity: 0.30045; pos pred: 0.66667
# # predictions_glm1_3 <- ifelse(prediction_values_glm1_3 > 0.2, 1, 0)
# # sensitivity: 0.6682; pos pred: 0.4508
# # Create Confusion Matrix
# CrossTable(x = test_3$yyes, y = predictions_glm1_3, propsq = FALSE)
# # Evaluate confusion matrix
# confusionMatrix(as.factor(predictions_glm1_3), as.factor(test_3$yyes), positive = "1") 
```

```{r}
# Model 2 - improve through excluding large p-value variables
glm_model2_3 <- glm(formula = yyes ~ . - age - jobadmin. - jobentrepreneur - jobhousemaid - jobretired - jobservices - maritalsingle - educationbasic.9y - housingyes - previous
                      , data = test_3, family = "binomial")
summary(glm_model2_3)
prediction_values_glm2_3 <- predict(glm_model2_3, newdata = test_3, type = "response")
predictions_glm2_3 <- ifelse(prediction_values_glm2_3 > 0.5, 1, 0)
# sensitivity: 0.30493 (>0.167); pos pred: 0.67327, use this model
# predictions_glm2_3 <- ifelse(prediction_values_glm2_3 > 0.3, 1, 0)
# sensitivity: 0.56054; pos pred: 0.51867
# predictions_glm2_3 <- ifelse(prediction_values_glm2_3 > 0.2, 1, 0)
# sensitivity: 0.6704; pos pred: 0.4517
CrossTable(x = test_3$yyes, y = predictions_glm2_3, propsq = FALSE)
confusion_matrix_glm2_3 <- confusionMatrix(as.factor(predictions_glm2_3), as.factor(test_3$yyes), positive = "1")
# Comparing for logistic regression, glm_model2_3 (binary prediction 0.5) is more accurate with sensitivity: 0.30493 (>0.167); pos pred: 0.67327
```

## Cluster 3 ANN

```{r, cache = TRUE}

library(neuralnet)
# # Model 1 (Base)
# model1_3 <- neuralnet(formula = yyes ~ ., data = train_3)
# plot(model1_3)
# prediction1_3 <- predict(model1_3, newdata = test_3)
# binaryprediction1_3 <- ifelse(prediction1_3 > 0.5, 1, 0)
# # Create Confusion Matrix
# library(gmodels)
# library(caret)
# CrossTable(x = test_3$yyes, y = binaryprediction1_3, propsq = FALSE)
# # Evaluate confusion matrix
# confusionMatrix(as.factor(binaryprediction1_3), as.factor(test_3$yyes), positive = "1")
# # sensitivity: 0.35874; pos pred: 0.55749
```

```{r}
# # Model 2 (Adding Nodes)
# model2_3 <- neuralnet(formula = yyes ~ ., data = train_3, hidden = 3)
# plot(model2_3)
# prediction2_3 <- predict(model2_3, newdata = test_3)
# binaryprediction2_3 <- ifelse(prediction2_3 > 0.5, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_3$yyes, y = binaryprediction2_3, propsq = FALSE)
# # Evaluate confusion matrix
# confusionMatrix(as.factor(binaryprediction2_3), as.factor(test_3$yyes), positive = "1")
# # sensitivity: 0.32287; pos pred: 0.52747 (both lower, will not use)
```

```{r}
library(neuralnet)
# Model 3 (Adding Layers)
model3_3 <- neuralnet(formula = yyes ~ ., data = train_3, hidden = c(1,3), threshold = 0.1, stepmax = 10^5)
plot(model3_3)
prediction3_3 <- predict(model3_3, newdata = test_3)
binaryprediction3_3 <- ifelse(prediction3_3 > 0.5, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_3$yyes, y = binaryprediction3_3, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction3_3), as.factor(test_3$yyes), positive = "1")
# sensitivity: 0.35650; pos pred: 0.55789 (improved compared to base)
```

```{r}
# # Model 4 (Adjusting Binary Prediction Definition)
# model4_3 <- neuralnet(formula = yyes ~ ., data = train_3)
# plot(model4_3)
# prediction4_3 <- predict(model4_3, newdata = test_3)
# binaryprediction4_3 <- ifelse(prediction4_3 > 0.4, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_3$yyes, y = binaryprediction4_3, propsq = FALSE)
# # Evaluate confusion matrix
# confusionMatrix(as.factor(binaryprediction4_3), as.factor(test_3$yyes), positive = "1")
# # sensitivity: 0.37892; pos pred: 0.54693
```

```{r}
# # Model 5 (Adjusting Binary Prediction Definition)
# model5_3 <- neuralnet(formula = yyes ~ ., data = train_3)
# plot(model5_3)
# prediction5_3 <- predict(model5_3, newdata = test_3)
# binaryprediction5_3 <- ifelse(prediction5_3 > 0.3, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_3$yyes, y = binaryprediction5_3, propsq = FALSE)
# # Evaluate confusion matrix
# confusionMatrix(as.factor(binaryprediction5_3), as.factor(test_3$yyes), positive = "1")
# # sensitivity: 0.46637; pos pred: 0.46532
```

```{r}
# # Model 6 (Adjusting Binary Prediction Definition)
# model6_3 <- neuralnet(formula = yyes ~ ., data = train_3)
# plot(model6_3)
# prediction6_3 <- predict(model6_3, newdata = test_3)
# binaryprediction6_3 <- ifelse(prediction6_3 > 0.2, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_3$yyes, y = binaryprediction6_3, propsq = FALSE)
# # Evaluate confusion matrix
# confusionMatrix(as.factor(binaryprediction6_3), as.factor(test_3$yyes), positive = "1")
# # sensitivity: 0.64126; pos pred: 0.43268 
# # Comparing all, model3_3 is most accurate, with sensitivity: 0.35650; pos pred: 0.55789
```

## Cluster 3 KNN

```{r}
train_labels_3 <- train_3$yyes
test_labels_3 <- test_3$yyes
train_KNN_3 <- train_3
train_KNN_3$yyes <- NULL
test_KNN_3 <- test_3
test_KNN_3$yyes <- NULL
library(class)
library(caret)
k_val1_3 <- 10
# sensitivity: 0.21076; pos pred: 0.54023
# k_val1_3 <- 3 
# sensitivity: 0.27354; pos pred: 0.47104
# k_val1_3 <- 4
# sensitivity: 0.25561; pos pred: 0.45600
# k_val1_3 <- 20
# sensitivity: 0.17040; pos pred: 0.63333
test_pred1_3 <- knn(train = train_KNN_3, test = test_KNN_3, cl = train_labels_3, k = k_val1_3)
#Evaluate model results
library(gmodels)
CrossTable(x = test_labels_3, y = test_pred1_3, 
           prop.chisq=FALSE)
# Evaluate confusion matrix
confusionMatrix(test_pred1_3, as.factor(test_labels_3), positive = "1")

# KNN model with k_val1_3 <- 10 is most accurate due to the balance of sensitivity and PPV, sensitivity: 0.21076; pos pred: 0.54023
```

## Cluster 3 Combined Model

```{r}
# Add up each row of the three predictor vectors for the three chosen models
added_predictors_3 <- as.numeric(test_pred1_3) + binaryprediction3_3[,1] + predictions_glm2_3
predictions_combined_model_3 <- ifelse(added_predictors_3 >= 2, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_3$yyes, y = predictions_combined_model_3, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_combined_model_3), as.factor(test_3$yyes), positive = "1")
# sensitivity: 0.6771; pos pred: 0.4461   
```

## Cluster 3 Additional Data Calculated for Chosen Model
```{r}
# based on comparing Sensitivity and Pos Pred Value
# glm: sensitivity: 0.30493; pos pred: 0.67327
# ANN: sensitivity: 0.35650; pos pred: 0.55789
# KNN: sensitivity: 0.21076; pos pred: 0.54023
# Combined: sensitivity: 0.6771; pos pred: 0.4461
# Will use glm model

# 1: Get number of potential calls (number of values in the test set of this cluster)
(num_potential_calls_3 <- nrow(test_3)) #2896

# 2: Get number of people we will call in this cluster 
# (number of people we predicted yyes to be 1 in this cluster)
# Add up values predicted to be 1 (from confusion matrix of chosen model for this cluster):
(num_calls_made_3 <- confusion_matrix_glm2_3$table[2] + confusion_matrix_glm2_3$table[4]) # 202

# 3: Calculate the success rate (number of people we will call 
#    (model predicted yyes to be 1) who actually have a yyes value of 1 
#    divided by the number of people we will call)
(success_rate_cluster3 <- confusion_matrix_glm2_3$table[4] / num_calls_made_3) # 0.6732673

# 4: Calculate the profit per 100 people:
# (100 * (# calls made / # potential calls made) * success rate * 6) -
# (100 * success rate * 1)
(profit_per_100_3 <- (100 * (num_calls_made_3 / num_potential_calls_3) * success_rate_cluster3 * 6) - (100 * success_rate_cluster3 * 1)) # -39.14994

# 5: Also, calculate what the profit is for this chosen model of this cluster
(profit_3 <- (num_calls_made_3 * success_rate_cluster3 * 6) - (num_calls_made_3 * 1)) # 614
```

```{}
Conclusion:
for cluster3, the most accurate model is glm_model2_3 (binary prediction 0.5, exclude large p-value variables)
number of potential calls are 2896
we will call 202
success rate is 0.6732673
profit per 100 people is -39.14994
profit for this cluster is 614
We will not call anyone in this cluster since the profit per 100 people is a negative value.
```


## Cluster 5

### Cluster 5 Training and Testing Data
```{r}
cluster_5_data <- tele_norm_k_means[tele_norm_k_means$cluster == 5,]
cluster_5_data$cluster <- NULL

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random_5 <- cluster_5_data[sample(nrow(cluster_5_data)),]

# Get 70% of data to be training data
n_train_5 <- round(nrow(cluster_5_data) * 0.7)
train_5 <- tele_random_5[1:n_train_5,]
# Get 30% of data to be testing data
test_5 <- tele_random_5[(n_train_5 + 1) :nrow(cluster_5_data),]
```

### Cluster 5 Logistic Regression
```{r}
library(gmodels)
library(caret)
# Model 1
# Create logistic regression model with all the predictor variables
glm_model_5 <- glm(formula = yyes ~ ., data = test_5, family = "binomial")
summary(glm_model_5)
# Get response values for testing data
prediction_values_glm5 <- predict(glm_model_5, newdata = test_5, type = "response")
# Get predictions for testing data
# predictions_glm5 <- ifelse(prediction_values_glm5 > 0.4, 1, 0)
# Sensitivity : 0.43011; Pos Pred Value : 0.68966
predictions_glm5 <- ifelse(prediction_values_glm5 > 0.5, 1, 0)
# Sensitivity: 0.35484; Pos Pred Value :  0.76744; use this model 
# predictions_glm5 <- ifelse(prediction_values_glm5 > 0.2, 1, 0)
# Sensitivity : 0.53763; Pos Pred Value : 0.50505, use this model 
# Create Confusion Matrix
CrossTable(x = test_5$yyes, y = predictions_glm5, propsq = FALSE)
# Evaluate confusion matrix
confusion_matrix_glm1_5 <- confusionMatrix(as.factor(predictions_glm5), as.factor(test_5$yyes), positive = "1")
```

```{r}
# #Model 2:
# # New model with variables that are statistically significant at an alpha level of 0.1
# glm_model2_5 <- glm(formula = yyes ~  + jobtechnician + contacttelephone + monthaug + monthmar + emp.var.rate  + cons.price.idx + euribor3m, data = test_5, family = "binomial")
# summary(glm_model2_5)
# prediction_values_glm2_5 <- predict(glm_model2_5, newdata = test_5, type = "response")
# predictions_glm2_5 <- ifelse(prediction_values_glm2_5 > 0.5, 1, 0)
# # Sensitivity : 0.36559  (>0.167); Pos Pred Value : 0.61818;  use this model  
# # predictions_glm2_5 <- ifelse(prediction_values_glm2_5 > 0.4, 1, 0)
# # Sensitivity : 0.37634 ; Pos Pred Value : 0.61404 
# # predictions_glm2_5 <- ifelse(prediction_values_glm2_5 > 0.2, 1, 0)
# # Sensitivity : 0.50538 ;  Pos Pred Value : 0.45192
# #predictions_glm2_5 <- ifelse(prediction_values_glm2_5 > 0.15, 1, 0)
# #Sensitivity : 0.51613 ; Pos Pred Value : 0.43636
# CrossTable(x = test_5$yyes, y = predictions_glm2_5, propsq = FALSE)
# confusionMatrix(as.factor(predictions_glm2_5), as.factor(test_5$yyes), positive = "1") 
# # Comparing for logistic regression, glm_model2_3 (binary prediction 0.5) is more accurate with sensitivity: 0.30493 (>0.167); pos pred: 0.67327
```

```{r}
# # Model 3:
# # New model that does not include variables who have large p-values (close to 1)
# glm_model3_5 <- glm(formula = yyes ~  - jobadmin. - jobblue.collar - jobentrepreneur - jobhousemaid - jobmanagement - jobretired - jobself.employed - jobservices - maritalunknown  - educationbasic.6y - educationbasic.9y - educationhigh.school- educationprofessional.course - educationprofessional.course - monthdec - emp.var.rate - pdaysdummy, data = test_5, family = "binomial")
# summary(glm_model3_5)
# # Get response values for testing data
# prediction_values_glm3_5 <- predict(glm_model2_5, newdata = test_5, type = "response")
# summary(prediction_values_glm3_5)
# #Get predictions for testing data
# # predictions_glm3_5 <- ifelse(prediction_values_glm3_5 > 0.12, 1, 0)
# # Sensitivity : 0.56989;  Pos Pred Value : 0.35333
# # predictions_glm3_5 <- ifelse(prediction_values_glm3_5 > 0.15, 1, 0)
# # Sensitivity : 0.51613 ; Pos Pred Value : 0.43636 
# #predictions_glm3_5 <- ifelse(prediction_values_glm3_5 > 0.2, 1, 0)
# # Sensitivity : 0.50538 ; Pos Pred Value : 0.45192
# predictions_glm3_5 <- ifelse(prediction_values_glm3_5 > 0.55, 1, 0)
# # Sensitivity : 0.26882 ; Pos Pred Value : 0.59524; use this model
# # Create Confusion Matrix
# CrossTable(x = test_5$yyes, y = predictions_glm3_5, propsq = FALSE)
# # Evaluate confusion matrix
# (confusion_matrix_glm3_5 <- confusionMatrix(as.factor(predictions_glm3_5), as.factor(test_5$yyes), positive = "1"))
# 
# # Note: glm_model1_5 is the best model of the three logistic regression models for cluster 5
# #       since it has a sensitivity > 0.167 and a larger pos pred value compared to the other final models
```


## Cluster 5 ANN

```{r, cache = TRUE}


# # Model 1 (Base line)
# model1_5 <- neuralnet(formula = yyes ~ ., data = train_5)
# plot(model1_5)
# prediction1_5 <- predict(model1_5, newdata = test_5)
# binaryprediction1_5 <- ifelse(prediction1_5 > 0.5, 1, 0)
# # Create Confusion Matrix
# library(gmodels)
# library(caret)
# CrossTable(x = test_5$yyes, y = binaryprediction1_5, propsq = FALSE)
# # Evaluate confusion matrix
# confusionMatrix(as.factor(binaryprediction1_5), as.factor(test_5$yyes), positive = "1")
# # Sensitivity : 0.31183; Pos Pred Value : 0.50877 
```



```{r}
# # Model 2 (Adding Nodes)
# model2_5 <- neuralnet(formula = yyes ~ ., data = train_5, hidden = 3)
# plot(model2_5)
# prediction2_5 <- predict(model2_5, newdata = test_5)
# binaryprediction2_5 <- ifelse(prediction2_5 > 0.5, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_5$yyes, y = binaryprediction2_5, propsq = FALSE)
# # Evaluate confusion matrix
# confusionMatrix(as.factor(binaryprediction2_5), as.factor(test_5$yyes), positive = "1")
# # Sensitivity : 0.32258; Pos Pred Value : 0.45455 
```


```{r}
# # Model 3 (Adding Layers)
# model3_5 <- neuralnet(formula = yyes ~ ., data = train_5, hidden = c(2,3), threshold = 0.01, stepmax = 10^5) 
# plot(model3_5)
# prediction3_5 <- predict(model3_5, newdata = test_5)
# binaryprediction3_5 <- ifelse(prediction3_5 > 0.5, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_5$yyes, y = binaryprediction3_5, propsq = FALSE)
# # Evaluate confusion matrix
# confusionMatrix(as.factor(binaryprediction3_5), as.factor(test_5$yyes), positive = "1")
# # Sensitivity : 0.24731;   Pos Pred Value : 0.44231

```

```{r}
# # Model 4 (Adjusting Binary Prediction Definition)
# model4_5 <- neuralnet(formula = yyes ~ ., data = train_5)
# plot(model4_5)
# prediction4_5 <- predict(model4_5, newdata = test_5)
# binaryprediction4_5 <- ifelse(prediction4_5 > 0.4, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_5$yyes, y = binaryprediction4_5, propsq = FALSE)
# # Evaluate confusion matrix
# confusionMatrix(as.factor(binaryprediction4_5), as.factor(test_5$yyes), positive = "1")
# #Sensitivity : 0.41935 ; Pos Pred Value : 0.49367 
```

```{r}
# # Model 5 (Adjusting Binary Prediction Definition)
# model5_5 <- neuralnet(formula = yyes ~ ., data = train_5)
# plot(model5_5)
# prediction5_5 <- predict(model5_5, newdata = test_5)
# binaryprediction5_5 <- ifelse(prediction5_5 > 0.3, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_5$yyes, y = binaryprediction5_5, propsq = FALSE)
# # Evaluate confusion matrix
# confusionMatrix(as.factor(binaryprediction5_5), as.factor(test_5$yyes), positive = "1")
# #Sensitivity : 0.48387; Pos Pred Value : 0.43689 
```

```{r}
library(neuralnet)
#Model 6 (Adjusting Binary Prediction Definition)
model6_5 <- neuralnet(formula = yyes ~ ., data = train_5)
plot(model6_5)
prediction6_5 <- predict(model6_5, newdata = test_5)
binaryprediction6_5 <- ifelse(prediction6_5 > 0.2, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_5$yyes, y = binaryprediction6_5, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(binaryprediction6_5), as.factor(test_5$yyes), positive = "1")
#Sensitivity : 0.36559 ; Pos Pred Value : 0.52308

# Comparing all, model6_5 is most accurate, with Sensitivity : 0.36559 ; Pos Pred Value : 0.52308
```




### Cluster 5 kNN

```{r}
train_labels_5 <- train_5$yyes
test_labels_5 <- test_5$yyes
train_KNN_5 <- train_5
train_KNN_5$yyes <- NULL
test_KNN_5 <- test_5
test_KNN_5$yyes <- NULL
library(class)
library(caret)
# k_val1_5 <- round(sqrt(nrow(train_5))) 
k_val1_5 <- 7
# Sensitivity : 0.30108 ; Pos Pred Value : 0.63636 --> use this threshold model 
# k_val1_5 <- 8 
# Sensitivity : 0.25806 ;  Pos Pred Value : 0.58537 
# k_val1_5 <- 4
# Sensitivity : 0.26882； Pos Pred Value : 0.49020 
# k_val1_5 <- 5
# Sensitivity : 0.26882；Pos Pred Value : 0.52083  
# k_val1_5 <- 6
# Sensitivity : 0.27957 ; Pos Pred Value : 0.56522 
# k_val1_5 <- 2
# Sensitivity : 0.27957；Pos Pred Value : 0.24528 
#k_val1_5 <- 3 
# Sensitivity : 0.26882 ; Pos Pred Value : 0.46296
test_pred1_5 <- knn(train = train_KNN_5, test = test_KNN_5, cl = train_labels_5, k = k_val1_5)
#Evaluate model results
library(gmodels)
CrossTable(x = test_labels_5, y = test_pred1_5, 
           prop.chisq=FALSE)
# Evaluate confusion matrix
(confusion_matrix_knn_5 <- confusionMatrix(test_pred1_5, as.factor(test_labels_5), positive = "1"))

# KNN model with k_val1_3 <- 7 is most accurate due to the balance of sensitivity and PPV, Sensitivity : 0.30108 ; Pos Pred Value : 0.63636
```


# Cluster 5 Combined Model

```{r}
# Add up each row of the three predictor vectors for the three chosen models
added_predictors_5 <- as.numeric(test_pred1_5) + binaryprediction6_5[,1] + predictions_glm5
predictions_combined_model_5 <- ifelse(added_predictors_5 >= 2, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_5$yyes, y = predictions_combined_model_5, propsq = FALSE)
# Evaluate confusion matrix
confusionMatrix(as.factor(predictions_combined_model_5), as.factor(test_5$yyes), positive = "1")
# Sensitivity : 0.50538 ; Pos Pred Value : 0.54651 
```


## Cluster 5 Additional Data Calculated for Chosen Model
```{r}
# based on comparing Sensitivity and Pos Pred Value
# glm: Sensitivity: 0.35484; Pos Pred Value : 0.76744
# ANN: Sensitivity : 0.36559 ; Pos Pred Value : 0.52308
# KNN: Sensitivity : 0.30108 ; Pos Pred Value : 0.63636
# Combined: Sensitivity : 0.50538 ; Pos Pred Value : 0.54651 
# Considering both Sensitivity and PPV, Will use glm model for cluster 5


# 1: Get number of potential calls (number of values in the test set of this cluster)
(num_potential_calls_5 <- nrow(test_5)) 
# 1037 in test set

# 2: Get number of people we will call in this cluster 
# (number of people we predicted yyes to be 1 in this cluster)
# Add up values predicted to be 1 (from confusion matrix of chosen model for this cluster):
(num_calls_made_5 <- confusion_matrix_glm1_5$table[2] + confusion_matrix_glm1_5$table[4]) 
# 43 calls made

# 3: Calculate the success rate (number of people we will call 
#    (model predicted yyes to be 1) who actually have a yyes value of 1 
#    divided by the number of people we will call)
(success_rate_cluster5 <- confusion_matrix_glm1_5$table[4] / num_calls_made_5) 
# 0.7674419

# 4: Calculate the profit per 100 people:
# (100 * (# calls made / # potential calls made) * success rate * 6) -
# (100 * success rate * 1)
(profit_per_100_5 <- (100 * (num_calls_made_5 / num_potential_calls_5) * success_rate_cluster5 * 6) - (100 * (num_calls_made_5 / num_potential_calls_5) * 1)) 
# 14.94696


# 5: Also, calculate what the profit is for this chosen model of this cluster
(profit_5 <- (num_calls_made_5 * success_rate_cluster5 * 6) - (num_calls_made_5 * (num_calls_made_5 / num_potential_calls_5) * 1)) 
#196.217
```

## Conclusion for Cluster 5
For cluster 5, we have chosen a logistic regression model since it has a positive profit per 100 people ($14.94696). In addition, the chosen logistic regression model for cluster 5 indicates to call 43 people, so it will not be very time consuming to call all these people.


## Chosen Combined Clusters' Additional Data Calculated
```{r}
# Do this in meeting 10-23
# Call people from: 
# chosen logistic regression model for cluster 1,
# chosen logistic regression model for cluster 2,
# chosen logistic regression model for cluster 5,
# cluster 4 (call all people in cluster 4)

# 1: Get number of potential calls
(num_potential_calls_total <- num_potential_calls_)
# 3088 in test set

# 2: Get number of people we will call in this cluster 
# (number of people we predicted yyes to be 1 in this cluster)
# Add up values predicted to be 1 (from confusion matrix of chosen model for this cluster):
(num_calls_made_ANN1 <- confusion_matrix_model2_ANN1$table[2] + confusion_matrix_model2_ANN1$table[4])
# 26 calls made

# 3: Calculate the success rate (number of people we will call 
#    (model predicted yyes to be 1) who actually have a yyes value of 1 
#    divided by the number of people we will call)
predicted_correctly_1s <- confusion_matrix_model2_ANN1$table[4]
(success_rate_clusterANN1 <- predicted_correctly_1s / num_calls_made_ANN1)
#0.4230769

# 4: Calculate the profit per 100 people:
# (100 * (# calls made / # potential calls made) * success rate * 6) -
# (100 * (# calls made/# potential calls made) * 1))
(profit_per_100_ANN1 <- (100 * (num_calls_made_ANN1 / num_potential_calls_ANN1)
                         * success_rate_clusterANN1 * 6) - 
                         (100 * (num_calls_made_ANN1 / num_potential_calls_ANN1) * 1))
#1.295337

# 5: Also, calculate what the profit is for this chosen model of this cluster
(profit_ANN1 <- (num_calls_made_ANN1 * success_rate_clusterANN1 * 6) - (num_calls_made_ANN1 * (num_calls_made_ANN1 / num_potential_calls_ANN1) * 1))
#65.78109

```







=======
## Conclusion

Do this in meeting 10-23